{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMHiSIzOEGCnkOoDTvnOPI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lllljjjjllllqqqq/123/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dj9jRsGYisD",
        "outputId": "3636ccd3-bbb8-4bd7-d207-3232e1e58f73"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7KpHPw9dLRQ",
        "outputId": "057286ab-8bb5-47ba-f45a-6e3c6253cf0e"
      },
      "source": [
        "!ls \"/content/drive/My Drive/tianchi_demo/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chinese_roberta_wwm_ext_L-12_H-768_A-12.zip\t    subs\n",
            "chinese_roberta_wwm_large_ext_L-24_H-1024_A-16.zip  TNEWS_train1128.csv\n",
            "OCEMOTION_train1128.csv\t\t\t\t    Untitled0.ipynb\n",
            "OCNLI_train1128.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdo84SZeZE4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c5de70-b422-4e52-8cf9-284edd6bcf5e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb 21 15:20:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWriqpZyZalR",
        "outputId": "e8722fca-4670-42d1-f3df-40b981749685"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=931d14cb5111d3a39df96b08a661767976a86b33b86b872b4601b59d51508ee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CocecVrqZatw",
        "outputId": "1c1aa1e0-c891-467b-e8d8-af37847c7359"
      },
      "source": [
        "import zipfile\r\n",
        "with zipfile.ZipFile('/content/drive/My Drive/tianchi_demo/chinese_roberta_wwm_ext_L-12_H-768_A-12.zip','r') as zzz:\r\n",
        "    print(zzz.namelist())\r\n",
        "    for x in zzz.namelist():\r\n",
        "      zzz.extract(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bert_config.json', 'bert_model.ckpt.data-00000-of-00001', 'bert_model.ckpt.index', 'bert_model.ckpt.meta', 'vocab.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psDrousKeAoa",
        "outputId": "fb322c06-a904-4b22-b662-b21752c937c2"
      },
      "source": [
        "file_name = wget.download('http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531841/OCEMOTION_train1128.csv')\r\n",
        "print(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OCEMOTION_train1128.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n3Zwxl9fA1m",
        "outputId": "b280f6d9-0e98-4e39-a8bc-e99aa8eebe56"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.meta  vocab.txt\n",
            "bert_model.ckpt.data-00000-of-00001  drive\n",
            "bert_model.ckpt.index\t\t     sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyT5G_6CfA4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cea8936-ede4-462f-d4ca-cd9d11971f83"
      },
      "source": [
        "pip install keras-bert #keras tensorflow pycaret fasttext"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=b3a0886b42d4456a65647aca66ef3f782d8c645218ff9d8db9e0e9953cb0be55\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12944 sha256=61ee445f41375555383d7e65272ac7b987198b0a0ba8da09e51187cc3fc7df1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=91e156e3d6cc6cc3d750550578d5041e814694f2b101d3e5b9c8a36eecf63be2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=af76f7ae6b018d7256260632e6127597752943ae3a354689608579154b4d0c4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5267 sha256=56d74ce57c9c3b7d48a5adb20a6269c99fe97fe76a2ade7cebbd4a73f479ea1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5625 sha256=c07e0b6c21b1d09edc04ae3f3018a61f307a15f1f1a922e861b8366a82df9c9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=14f96b4b0091d5be783b6855a041267bffa3bc17f2c825b7a2b583d65ce6678c\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=64c56237a857f80b402254c9674eb1d1a7b99ff4c2f82296027a6fb2d7fbd0f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqKZwsWfA7G"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os, sys\r\n",
        "import codecs, gc\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\r\n",
        "from keras.metrics import top_k_categorical_accuracy\r\n",
        "from keras.layers import *\r\n",
        "from keras.callbacks import *\r\n",
        "from keras.models import Model\r\n",
        "import keras.backend as K\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.utils import to_categorical\r\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv6848D2fA9r"
      },
      "source": [
        "#路径配置\r\n",
        "path = '..'\r\n",
        "docker_data_path = '/tcdata/'\r\n",
        "\r\n",
        "'''\r\n",
        "路径说明：\r\n",
        "|--tianchi-nlp\r\n",
        "​\t |-- code\r\n",
        "\t|-- data\r\n",
        "\t|-- subs\r\n",
        "  |-- chinese_roberta_wwm_large_ext_L-24_H-1024_A-16\r\n",
        "\r\n",
        "其中：\r\n",
        "/code 保存代码\r\n",
        "/data #保存数据\r\n",
        "/subs #保存数据\r\n",
        "/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16 #bert路径\r\n",
        "'''\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#一些调优参数\r\n",
        "er_patience = 2 #early_stopping patience\r\n",
        "lr_patience = 5 #ReduceLROnPlateau patience\r\n",
        "max_epochs = 3 #epochs\r\n",
        "lr_rate = 3e-5 #learning rate\r\n",
        "batch_sz =16  #batch_size 4\r\n",
        "maxlen = 200  #设置序列长度为，base模型要保证序列长度不超过512\r\n",
        "lr_factor = 0.85 #ReduceLROnPlateau factor，lr*=factor\r\n",
        "maxlentext1 = 200 #选择ocnli子句一的长度\r\n",
        "n_folds = 10   #设置验证集的占比: 1/n_folds\r\n",
        "#ReduceLROnPlateau其他参数：https://keras.io/zh/callbacks/#reducelronplateau\r\n",
        "\r\n",
        "#path_bert = path + '/chinese_rbt6_L-6_H-768_A-12/'\r\n",
        "#预训练好的模型相关路径\r\n",
        "config_path ='/content/bert_config.json'\r\n",
        "dict_path   ='/content/vocab.txt'\r\n",
        "checkpoint_path ='/content/bert_model.ckpt'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUgeeCfkfBAS"
      },
      "source": [
        "#数据读取\r\n",
        "path='/content/drive/MyDrive/tianchi_demo/'\r\n",
        "#将ocnli中content1[0:maxlentext1]+content2作为ocnli的content输入\r\n",
        "times_train = pd.read_csv(path + 'TNEWS_train1128.csv',    sep='\\t', header=None, names=('id', 'content', 'label')).astype(str)\r\n",
        "ocemo_train = pd.read_csv(path + 'OCEMOTION_train1128.csv',sep='\\t', header=None, names=('id', 'content', 'label')).astype(str)\r\n",
        "ocnli_train = pd.read_csv(path + 'OCNLI_train1128.csv',    sep='\\t', header=None, names=('id', 'content1', 'content2', 'label')).astype(str)\r\n",
        "ocnli_train['content'] = ocnli_train['content1'] + ocnli_train['content2']#.apply( lambda x: x[:maxlentext1] )\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scpWVCHHgj0M",
        "outputId": "9dd401d6-11c3-446e-9ee8-ed0a9fe09652"
      },
      "source": [
        "#EDA分析\r\n",
        "#子句文本长度统计分析\r\n",
        "times_train['content'].str.len().describe(percentiles=[.99, .999, .9999]),ocemo_train['content'].str.len().describe(percentiles=[.99, .999, .9999])\\\r\n",
        ",ocnli_train['content1'].str.len().describe(percentiles=[.99, .999, .9999]),ocnli_train['content2'].str.len().describe(percentiles=[.99, .999, .9999])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(count     63360.000000\n",
              " mean         22.171086\n",
              " std           7.334206\n",
              " min           2.000000\n",
              " 50%          22.000000\n",
              " 99%          39.000000\n",
              " 99.9%        48.000000\n",
              " 99.99%       58.000000\n",
              " max         145.000000\n",
              " Name: content, dtype: float64, count     35315.000000\n",
              " mean         48.214328\n",
              " std          84.391942\n",
              " min           3.000000\n",
              " 50%          34.000000\n",
              " 99%         142.000000\n",
              " 99.9%       172.000000\n",
              " 99.99%     1040.953200\n",
              " max       12326.000000\n",
              " Name: content, dtype: float64, count     48778.000000\n",
              " mean         24.174607\n",
              " std          11.515428\n",
              " min           8.000000\n",
              " 50%          22.000000\n",
              " 99%          50.000000\n",
              " 99.9%        50.000000\n",
              " 99.99%       50.000000\n",
              " max          50.000000\n",
              " Name: content1, dtype: float64, count      48778.000000\n",
              " mean          15.828529\n",
              " std          977.396848\n",
              " min            2.000000\n",
              " 50%           10.000000\n",
              " 99%           27.000000\n",
              " 99.9%         40.000000\n",
              " 99.99%        54.000000\n",
              " max       215874.000000\n",
              " Name: content2, dtype: float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqzJ5izXgj3Y",
        "outputId": "368613e4-0b92-4c49-950c-528420bc4f77"
      },
      "source": [
        "#构造输入、标签数据\r\n",
        "#合并三个任务的训练、测试数据，将三任务转为单个任务\r\n",
        "train_df = pd.concat([times_train, ocemo_train, ocnli_train[['id','content', 'label']]], axis=0).copy()\r\n",
        "#LabelEncoder处理标签，因为label需要从0开始\r\n",
        "encode_label = LabelEncoder()\r\n",
        "train_df['label'] = encode_label.fit_transform(train_df['label'].apply(str))\r\n",
        "\r\n",
        "###采用分层抽样的方式，从训练集中抽取10%作为验证集\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=222)\r\n",
        "\r\n",
        "X_trn = pd.DataFrame()\r\n",
        "X_val = pd.DataFrame()\r\n",
        "\r\n",
        "for train_index, test_index in skf.split(train_df.copy(), train_df['label']):\r\n",
        "  X_trn, X_val = train_df.iloc[train_index], train_df.iloc[test_index]\r\n",
        "  break\r\n",
        "###\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#训练数据、测试数据和标签转化为模型输入格式\r\n",
        "n_cls = len( train_df['label'].unique() )\r\n",
        "\r\n",
        "#训练集每行的content、label转为tuple存入list，再转为numpy array\r\n",
        "TRN_LIST = []\r\n",
        "for data_row in X_trn.iloc[:].itertuples():\r\n",
        "  TRN_LIST.append((data_row.content, to_categorical(data_row.label, n_cls)))\r\n",
        "TRN_LIST = np.array(TRN_LIST)\r\n",
        "\r\n",
        "#验证集每行的content、label转为tuple存入list，再转为numpy array\r\n",
        "VAL_LIST = []\r\n",
        "for data_row in X_val.iloc[:].itertuples():\r\n",
        "  VAL_LIST.append((data_row.content, to_categorical(data_row.label, n_cls)))\r\n",
        "VAL_LIST = np.array(VAL_LIST)\r\n",
        "\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9cPqOjvkYmv",
        "outputId": "5d1dd85b-8303-4883-8ce4-21c225c575ae"
      },
      "source": [
        "#EDA分析\r\n",
        "#标签占比统计分析\r\n",
        "train_df['label'].value_counts() / train_df.shape[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     0.113467\n",
              "0     0.109940\n",
              "17    0.107397\n",
              "23    0.084603\n",
              "21    0.060318\n",
              "10    0.047771\n",
              "6     0.041749\n",
              "4     0.039918\n",
              "13    0.039036\n",
              "8     0.033292\n",
              "3     0.032668\n",
              "5     0.032268\n",
              "11    0.029487\n",
              "19    0.029481\n",
              "9     0.027690\n",
              "18    0.027588\n",
              "12    0.027541\n",
              "16    0.027460\n",
              "22    0.027412\n",
              "15    0.022923\n",
              "7     0.016853\n",
              "2     0.008993\n",
              "24    0.006097\n",
              "20    0.004001\n",
              "14    0.002048\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp3V4SmhkaNC"
      },
      "source": [
        "#数据预处理\r\n",
        "#让每条文本的长度相同，用0填充\r\n",
        "def seq_padding(X, padding=0):\r\n",
        "  L = [len(x) for x in X]\r\n",
        "  ML = max(L)\r\n",
        "  return np.array([  np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X  ])\r\n",
        "\r\n",
        "#将词表中的词编号转换为字典\r\n",
        "token_dict = {}\r\n",
        "with codecs.open(dict_path, 'r', 'utf8') as reader:\r\n",
        "  for line in reader:\r\n",
        "    token = line.strip()\r\n",
        "    token_dict[token] = len(token_dict)\r\n",
        "\r\n",
        "#重写tokenizer        \r\n",
        "class OurTokenizer(Tokenizer):\r\n",
        "  def _tokenize(self, text):\r\n",
        "    R = []\r\n",
        "    for c in text:\r\n",
        "      if c in self._token_dict:\r\n",
        "        R.append(c)\r\n",
        "      elif self._is_space(c):\r\n",
        "        R.append('[unused1]') # 用[unused1]来表示空格类字符\r\n",
        "      else:\r\n",
        "        R.append('[UNK]')     # 不在列表的字符用[UNK]表示\r\n",
        "    return R\r\n",
        "\r\n",
        "tokenizer = OurTokenizer(token_dict)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8IQ1jexkaP9"
      },
      "source": [
        "#data_generator只是一种为了节约内存的数据方式\r\n",
        "class data_generator:\r\n",
        "  global batch_sz\r\n",
        "  def __init__(self, data, batch_size=batch_sz, shuffle=True):#此处修改batch_size\r\n",
        "    self.data = data\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.shuffle = shuffle\r\n",
        "    self.steps = len(self.data) // self.batch_size\r\n",
        "    if len(self.data) % self.batch_size != 0:\r\n",
        "      self.steps += 1\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.steps\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    while True:\r\n",
        "      idxs = list(range(len(self.data)))\r\n",
        "      if self.shuffle:\r\n",
        "        np.random.shuffle(idxs)\r\n",
        "      X1, X2, Y = [], [], []\r\n",
        "      for i in idxs:\r\n",
        "        d = self.data[i]\r\n",
        "        text = d[0][: maxlen]\r\n",
        "        #text2 = d[0][maxlentext1:maxlen]\r\n",
        "        x1, x2 = tokenizer.encode(first=text)#1, second=text2)#修改成输入两个子句\r\n",
        "\r\n",
        "        y = d[1]\r\n",
        "        X1.append(x1)\r\n",
        "        X2.append(x2)\r\n",
        "        Y.append([y])\r\n",
        "        if len(X1) == self.batch_size or i == idxs[-1]:\r\n",
        "          X1 = seq_padding(X1)\r\n",
        "          X2 = seq_padding(X2)\r\n",
        "          Y = seq_padding(Y)\r\n",
        "          yield [X1, X2], Y[:, 0, :]\r\n",
        "          [X1, X2, Y] = [], [], []"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfTyMm2TkaSz"
      },
      "source": [
        "\r\n",
        "#定义计算每个batch的f1\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "def f1(y_true, y_pred):\r\n",
        "  def recall(y_true, y_pred):\r\n",
        "    \"\"\"Recall metric.\r\n",
        "\r\n",
        "    Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "    Computes the recall, a metric for multi-label classification of\r\n",
        "    how many relevant items are selected.\r\n",
        "    \"\"\"\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "  def precision(y_true, y_pred):\r\n",
        "    \"\"\"Precision metric.\r\n",
        "\r\n",
        "    Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "    Computes the precision, a metric for multi-label classification of\r\n",
        "    how many selected items are relevant.\r\n",
        "    \"\"\"\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    return precision\r\n",
        "  precision = precision(y_true, y_pred)\r\n",
        "  recall = recall(y_true, y_pred)\r\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdsS1BtTkh4z"
      },
      "source": [
        "#bert模型搭建与配置\r\n",
        "def build_bert(nclass):\r\n",
        "  global lr_rate\r\n",
        "  bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)  #加载预训练模型\r\n",
        "\r\n",
        "  for l in bert_model.layers:\r\n",
        "    l.trainable = True\r\n",
        "\r\n",
        "  x1_in = Input(shape=(None,))\r\n",
        "  x2_in = Input(shape=(None,))\r\n",
        "\r\n",
        "  x = bert_model([x1_in, x2_in])\r\n",
        "  x = Lambda(lambda x: x[:, 0])(x) #取出[CLS]对应的向量用来做分类\r\n",
        "  p = Dense(nclass, activation='softmax')(x) #接dense层softmax输出\r\n",
        "\r\n",
        "  model = Model([x1_in, x2_in], p)\r\n",
        "  model.compile(loss='categorical_crossentropy',#损失函数\r\n",
        "                optimizer=Adam(lr_rate),    #优化器选择及学习率\r\n",
        "                metrics=['accuracy', f1])   #评价函数\r\n",
        "\r\n",
        "  print(model.summary())\r\n",
        "  return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOCbN33Lkh7R"
      },
      "source": [
        "#交叉验证训练和测试模型\r\n",
        "def run_nocv(nfold, trn_data, val_data, data_labels, n_cls):\r\n",
        "  global er_patience\r\n",
        "  global lr_patience\r\n",
        "  global max_epochs\r\n",
        "  global f1metrics\r\n",
        "  global lr_factor\r\n",
        "  #test_model_pred = np.zeros((len(data_test), n_cls))\r\n",
        "\r\n",
        "  model = build_bert(n_cls)\r\n",
        "  #下行代码用于加载保存的权重继续训练\r\n",
        "  #model.load_weights(path + '/subs/model.epoch02_val_loss1.0146_val_acc0.6479_val_f10.6359.hdf5')\r\n",
        "  \r\n",
        "  early_stopping = EarlyStopping(monitor= \"val_f1\", patience=er_patience) #早停法，防止过拟合 #'val_accuracy'\r\n",
        "  plateau = ReduceLROnPlateau(monitor=\"val_f1\", verbose=1, mode='max', factor=lr_factor, patience=lr_patience) #当评价指标不在提升时，减少学习率 #\"val_accuracy\"\r\n",
        "  checkpoint = ModelCheckpoint(path + \"subs/model.epoch{epoch:02d}_val_loss{val_loss:.4f}_val_acc{val_accuracy:.4f}_val_f1{val_f1:.4f}.hdf5\", monitor=\"val_f1\", verbose=2, save_best_only=True, mode='max', save_weights_only=True) #保存val_f1最好的模型权重#'val_acc'\r\n",
        "\r\n",
        "  #训练跟验证集可shuffle打乱，测试集不可打乱也无必要（不然在生成线上结果时没法跟ID对应上）\r\n",
        "  #其他\r\n",
        "  train_D = data_generator(trn_data, shuffle=True)\r\n",
        "  valid_D = data_generator(val_data, shuffle=True)\r\n",
        "  #test_D = data_generator(data_test, shuffle=False)\r\n",
        "  \r\n",
        "  #模型训练\r\n",
        "  model.fit_generator(\r\n",
        "      train_D.__iter__(),\r\n",
        "      steps_per_epoch=len(train_D),\r\n",
        "      epochs=max_epochs,\r\n",
        "      validation_data=valid_D.__iter__(),\r\n",
        "      validation_steps=len(valid_D),\r\n",
        "      callbacks=[early_stopping, plateau, checkpoint],\r\n",
        "  )\r\n",
        "  model.save(path+'bert_model.hdf5')\r\n",
        "  #模型预测\r\n",
        "  \r\n",
        "  test_model_pred = model.predict_generator(train_D.__iter__(), steps=len(test_D), verbose=1)\r\n",
        "  train_model_pred = test_model_pred#model.predict(train_D.__iter__(), steps=len(train_D), verbose=1)\r\n",
        " \r\n",
        "  del model\r\n",
        "  gc.collect()   #清理内存\r\n",
        "  K.clear_session() #clear_session就是清除一个session\r\n",
        "\r\n",
        "  return test_model_pred, train_model_pred"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW1QDF6gI5Tw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyIPGOkfkh-M",
        "outputId": "4a10be6c-9ecd-4045-f8ca-7269eb4faa22"
      },
      "source": [
        "#'''\r\n",
        "#此cell代码为训练代码\r\n",
        "cvs = 1\r\n",
        "#输出为numpy array格式的25列概率\r\n",
        "test_model_pred, train_model_pred = run_nocv(cvs, TRN_LIST, VAL_LIST, None, n_cls)\r\n",
        "\r\n",
        "#将结果转为DataFrame格式\r\n",
        "preds_tst_df = pd.DataFrame(test_model_pred)\r\n",
        "preds_trn_df = pd.DataFrame(train_model_pred)\r\n",
        "\r\n",
        "#再将range(0,25)做encode_label逆变换作为该DataFrame的列名\r\n",
        "preds_col_names = encode_label.inverse_transform( range(0,n_cls) )\r\n",
        "preds_tst_df.columns = preds_col_names\r\n",
        "preds_trn_df.columns = preds_col_names\r\n",
        "\r\n",
        "#保存输出概率，将该输出概率输入树模型还能提升分数，大概4个千分点\r\n",
        "#preds_tst_df.to_csv(path + \"/subs/tstsprob.csv\", index=None)\r\n",
        "#preds_trn_df.to_csv(path + \"/subs/trnsprob.csv\", index=None)\r\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, None, 768)    101677056   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 768)          0           model_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 25)           19225       lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 101,696,281\n",
            "Trainable params: 101,696,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1908/8295 [=====>........................] - ETA: 50:39 - loss: 1.6125 - accuracy: 0.4554 - f1: 0.3792"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}